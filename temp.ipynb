{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "7e3dcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "0305cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = torch.tensor([2.0], requires_grad=True)\n",
    "# b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# # Input\n",
    "# x = torch.tensor([3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "fcc81597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forward pass\n",
    "# y_pred = w * x + b\n",
    "# print(\"y_pred:\", {y_pred.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "649525e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss\n",
    "# y_true = torch.tensor([10.0])\n",
    "# loss = (y_pred - y_true) ** 2\n",
    "# print(\"Loss:\", {loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "625f0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Avant backward: w.grad =\", w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "9e4119fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "a4ff1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Après backward: w.grad =\", w.grad)\n",
    "# print(\"Apres backward: b.grad =\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "62da28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.01\n",
    "# with torch.no_grad(): # Désactiver le tracking des gradients pour l'update w = w + alpha * w\n",
    "#     w -= learning_rate * w.grad\n",
    "#     b -= learning_rate * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "bd547104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Nouveau poids : w =\", w, \"b =\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "c7cd113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Réinitialiser les gradients\n",
    "# w.grad.zero_()\n",
    "# b.grad.zero_()\n",
    "# print(\"Après zero_(): w.grad =\", w.grad, \"b.grad =\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "efea9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# perceptron = nn.Linear(1, 1)\n",
    "\n",
    "# # Voir les poids initiaux\n",
    "# print(\"Poids :\", perceptron.weight)\n",
    "# print(\"Biais :\", perceptron.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "b2015071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forward\n",
    "# x = torch.tensor([3.0]) # Shape (batch_size, features)\n",
    "# print(f\"input : {x}\")\n",
    "# y_pred = perceptron(x)\n",
    "# print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "7bc0fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss\n",
    "# y_true = torch.tensor([10.0])\n",
    "# criterion = nn.MSELoss()\n",
    "# loss = criterion(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "4dd9f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "1cdd0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Backward\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "7b96c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Gradient de poids : {perceptron.weight.grad}\")\n",
    "# print(f\"Gradient de biais : {perceptron.bias.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "c9de3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# optimizer = optim.SGD(perceptron.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "1030f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "90abcca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Nouveau poids :\", perceptron.weight)\n",
    "# print(\"Nouveau biais :\", perceptron.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "4a1c707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "87daca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Poids après zero_grad : {perceptron.weight.grad}\")\n",
    "# print(f\"Biais après zero_grad : {perceptron.bias.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "dcf98ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(2, 4)\n",
    "#         self.fc2 = nn.Linear(4, 3)\n",
    "#         self.fc3 = nn.Linear(3, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "79448116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "d73c7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "eae47c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.tensor([1.0, 2.0])\n",
    "# y = torch.tensor([5.0])\n",
    "# y_pred = model(x)\n",
    "# loss = nn.MSELoss()(y_pred, y)\n",
    "\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "d17a0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize, RandomHorizontalFlip, RandomCrop, ColorJitter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "02461311",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_training = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomCrop(28, padding=4),\n",
    "    ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,),(0.3081,))\n",
    "])\n",
    "\n",
    "transform_testing = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,),(0.3081,))\n",
    "])\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_training\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "d7553f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "32c79140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n",
      "Pixel array (0..1):\n",
      "[[0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.02352941 0.05882353 0.05882353 0.05882353\n",
      "  0.33333334 0.35686275 0.45490193 0.08235294 0.43137252 0.65882355\n",
      "  0.6392157  0.33333334 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.09019609 0.10588235\n",
      "  0.2509804  0.40392154 0.4470588  0.6509804  0.6509804  0.6509804\n",
      "  0.6509804  0.6509804  0.58431375 0.45098037 0.6509804  0.627451\n",
      "  0.50980395 0.1764706  0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.13725491 0.6156863  0.6509804\n",
      "  0.6509804  0.6509804  0.6509804  0.6509804  0.6509804  0.6509804\n",
      "  0.6509804  0.6509804  0.24705882 0.2235294  0.2235294  0.15686275\n",
      "  0.11372549 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.05882353 0.56862754 0.6509804\n",
      "  0.6509804  0.6509804  0.6509804  0.6509804  0.5137255  0.47450978\n",
      "  0.6392157  0.62352943 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.21568628 0.4078431\n",
      "  0.28627452 0.6509804  0.6509804  0.5294118  0.04313725 0.01568628\n",
      "  0.1254902  0.40392154 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.04705883\n",
      "  0.01568628 0.40392154 0.6509804  0.24313726 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.36862746 0.6509804  0.49411762 0.01960784 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.04313725 0.49411762 0.6509804  0.1882353  0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.10196079 0.62352943 0.58431375 0.41960782\n",
      "  0.28627452 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.21960784 0.61960787 0.6509804\n",
      "  0.6509804  0.3137255  0.07843138 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.1254902  0.48627448\n",
      "  0.6509804  0.6509804  0.39215684 0.08235294 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.05490196\n",
      "  0.24705882 0.6509804  0.6509804  0.49019605 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.6431373  0.6509804  0.6431373  0.1764706  0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.12941177 0.34509805\n",
      "  0.47843134 0.6509804  0.6509804  0.5372549  0.01960784 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.11372549 0.38823527 0.59215695 0.6509804\n",
      "  0.6509804  0.6509804  0.64705884 0.47450978 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.07450981 0.3019608  0.57254905 0.6509804  0.6509804  0.6509804\n",
      "  0.6509804  0.52156866 0.21176472 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.07058824 0.18039216\n",
      "  0.5529412  0.6509804  0.6509804  0.6509804  0.6509804  0.5137255\n",
      "  0.21960784 0.01960784 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.05882353 0.4470588  0.56862754 0.6509804\n",
      "  0.6509804  0.6509804  0.6509804  0.50980395 0.21568628 0.03529412\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.15294118 0.45098037 0.5882353  0.6509804  0.6509804  0.6509804\n",
      "  0.6509804  0.6313726  0.34901962 0.04313725 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.35686275 0.6509804  0.6509804  0.6509804  0.54901963 0.35686275\n",
      "  0.34901962 0.05490196 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]\n",
      " [0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628 0.01568628 0.01568628\n",
      "  0.01568628 0.01568628 0.01568628 0.01568628]]\n",
      "White pixels (>=0.5): 84, Black pixels (<0.5): 700, total: 784\n"
     ]
    }
   ],
   "source": [
    "# Inspect one transformed image: pixel values and white/black counts\n",
    "sample_img, sample_label = training_data[0]\n",
    "print(f\"Label: {sample_label}\")\n",
    "img_denorm = sample_img * 0.3081 + 0.1307  # undo normalization back to [0,1] approx\n",
    "img_np = img_denorm.squeeze().numpy()\n",
    "print(\"Pixel array (0..1):\")\n",
    "print(img_np)\n",
    "white = (img_np >= 0.5).sum()\n",
    "black = (img_np < 0.5).sum()\n",
    "print(f\"White pixels (>=0.5): {white}, Black pixels (<0.5): {black}, total: {img_np.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "315d9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "<built-in method size of Tensor object at 0x0000020256455430>\n"
     ]
    }
   ],
   "source": [
    "print(training_data.classes)\n",
    "print(training_data.data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "ad10a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(28*28, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 10),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.fc(x)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "4bc78dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "# model = NeuralNetwork().to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "491a756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(1, 28, 28, device=device)\n",
    "# logits = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "3f262e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "e065d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "3cf926d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = pred_probab.argmax(dim=1)\n",
    "# y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "cf529cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "e7dd7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(dataloader, model, loss_fn, optimizer):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     model.train()\n",
    "#     for batch_idx, batch_value in enumerate(dataloader):\n",
    "#         X, y = batch_value\n",
    "#         X, y = X.to(device), y.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         pred = model(X)\n",
    "#         loss = loss_fn(pred, y)\n",
    "\n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             loss, current = loss.item(), (batch_idx+1) * len(X)\n",
    "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "4b5c908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(dataloader, model, loss_fn):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     model.eval()\n",
    "#     test_loss, correct = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             pred = model(X)\n",
    "#             test_loss += loss_fn(pred, y).item()\n",
    "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "#     test_loss /= num_batches\n",
    "#     correct /= size\n",
    "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "d269dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train(training_dataloader, model, loss_fn, optimizer)\n",
    "#     test(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "d891bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_inputs = (torch.randn(1, 1, 28, 28),)\n",
    "# model.to('cpu')\n",
    "# onnx_program = torch.onnx.export(model, example_inputs, dynamo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "2efb6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_program.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "8f2279db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "b7f2d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 10)  # 28x28 input -> pool -> pool -> conv => 7x7 feature map\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "2b70b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=25088, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "96b2e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1, 28, 28, device=device)\n",
    "logits = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "507a185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0878, 0.1342, 0.1429, 0.0737, 0.0580, 0.0426, 0.1203, 0.1800, 0.0591,\n",
       "         0.1013]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "4e5e3305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pred_probab.argmax(dim=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "017ac048",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "2d17a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "d137b6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logging to runs/mnist_cnn_20251211-162230\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "log_dir = f\"runs/mnist_cnn_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "print(f\"TensorBoard logging to {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "67ead34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_graph(model, example_inputs)\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "e3065a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, writer, epoch_index):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0.0\n",
    "    running_seen = 0\n",
    "    for batch_idx, batch_value in enumerate(dataloader):\n",
    "        X, y = batch_value\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = X.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        running_seen += batch_size\n",
    "        global_step = epoch_index * len(dataloader) + batch_idx\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            writer.add_scalar('Training/Batch_Loss', loss.item(), global_step)\n",
    "            writer.add_scalar('Training/Batch_Accuracy', running_correct / running_seen, global_step)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss_val, current = loss.item(), (batch_idx+1) * len(X)\n",
    "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5}]\")\n",
    "\n",
    "    epoch_loss = running_loss / size\n",
    "    epoch_accuracy = running_correct / size\n",
    "    writer.add_scalar('Training/Epoch_Loss', epoch_loss, epoch_index)\n",
    "    writer.add_scalar('Training/Epoch_Accuracy', epoch_accuracy, epoch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "e08ccccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, writer, epoch_index):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    writer.add_scalar('Test/Accuracy', correct, epoch_index)\n",
    "    writer.add_scalar('Test/Avg_Loss', test_loss, epoch_index)\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "94a4eec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\Documents\\IIM\\DEEP_LEARNING\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.369785  [   64/60000]\n",
      "loss: 0.981711  [ 6464/60000]\n",
      "loss: 0.981711  [ 6464/60000]\n",
      "loss: 0.750521  [12864/60000]\n",
      "loss: 0.750521  [12864/60000]\n",
      "loss: 0.432702  [19264/60000]\n",
      "loss: 0.432702  [19264/60000]\n",
      "loss: 0.251398  [25664/60000]\n",
      "loss: 0.251398  [25664/60000]\n",
      "loss: 0.300395  [32064/60000]\n",
      "loss: 0.300395  [32064/60000]\n",
      "loss: 0.451014  [38464/60000]\n",
      "loss: 0.451014  [38464/60000]\n",
      "loss: 0.471761  [44864/60000]\n",
      "loss: 0.471761  [44864/60000]\n",
      "loss: 0.460873  [51264/60000]\n",
      "loss: 0.460873  [51264/60000]\n",
      "loss: 0.280067  [57664/60000]\n",
      "loss: 0.280067  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.483974 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.483974 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.687028  [   64/60000]\n",
      "loss: 0.687028  [   64/60000]\n",
      "loss: 0.368583  [ 6464/60000]\n",
      "loss: 0.368583  [ 6464/60000]\n",
      "loss: 0.199836  [12864/60000]\n",
      "loss: 0.199836  [12864/60000]\n",
      "loss: 0.033065  [19264/60000]\n",
      "loss: 0.033065  [19264/60000]\n",
      "loss: 0.427788  [25664/60000]\n",
      "loss: 0.427788  [25664/60000]\n",
      "loss: 0.398985  [32064/60000]\n",
      "loss: 0.398985  [32064/60000]\n",
      "loss: 0.056614  [38464/60000]\n",
      "loss: 0.056614  [38464/60000]\n",
      "loss: 0.035706  [44864/60000]\n",
      "loss: 0.035706  [44864/60000]\n",
      "loss: 0.167067  [51264/60000]\n",
      "loss: 0.167067  [51264/60000]\n",
      "loss: 0.147615  [57664/60000]\n",
      "loss: 0.147615  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.454340 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.454340 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.328214  [   64/60000]\n",
      "loss: 0.328214  [   64/60000]\n",
      "loss: 0.129417  [ 6464/60000]\n",
      "loss: 0.129417  [ 6464/60000]\n",
      "loss: 0.123308  [12864/60000]\n",
      "loss: 0.123308  [12864/60000]\n",
      "loss: 0.249274  [19264/60000]\n",
      "loss: 0.249274  [19264/60000]\n",
      "loss: 0.086435  [25664/60000]\n",
      "loss: 0.086435  [25664/60000]\n",
      "loss: 0.078283  [32064/60000]\n",
      "loss: 0.078283  [32064/60000]\n",
      "loss: 0.157013  [38464/60000]\n",
      "loss: 0.157013  [38464/60000]\n",
      "loss: 0.068175  [44864/60000]\n",
      "loss: 0.068175  [44864/60000]\n",
      "loss: 0.229069  [51264/60000]\n",
      "loss: 0.229069  [51264/60000]\n",
      "loss: 0.132598  [57664/60000]\n",
      "loss: 0.132598  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.153008 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.153008 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.095811  [   64/60000]\n",
      "loss: 0.095811  [   64/60000]\n",
      "loss: 0.066746  [ 6464/60000]\n",
      "loss: 0.066746  [ 6464/60000]\n",
      "loss: 0.079924  [12864/60000]\n",
      "loss: 0.079924  [12864/60000]\n",
      "loss: 0.115271  [19264/60000]\n",
      "loss: 0.115271  [19264/60000]\n",
      "loss: 0.046526  [25664/60000]\n",
      "loss: 0.046526  [25664/60000]\n",
      "loss: 0.047942  [32064/60000]\n",
      "loss: 0.047942  [32064/60000]\n",
      "loss: 0.345667  [38464/60000]\n",
      "loss: 0.345667  [38464/60000]\n",
      "loss: 0.116188  [44864/60000]\n",
      "loss: 0.116188  [44864/60000]\n",
      "loss: 0.359388  [51264/60000]\n",
      "loss: 0.359388  [51264/60000]\n",
      "loss: 0.112530  [57664/60000]\n",
      "loss: 0.112530  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.087174 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.087174 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.200871  [   64/60000]\n",
      "loss: 0.200871  [   64/60000]\n",
      "loss: 0.075294  [ 6464/60000]\n",
      "loss: 0.075294  [ 6464/60000]\n",
      "loss: 0.144117  [12864/60000]\n",
      "loss: 0.144117  [12864/60000]\n",
      "loss: 0.023039  [19264/60000]\n",
      "loss: 0.023039  [19264/60000]\n",
      "loss: 0.054145  [25664/60000]\n",
      "loss: 0.054145  [25664/60000]\n",
      "loss: 0.158390  [32064/60000]\n",
      "loss: 0.158390  [32064/60000]\n",
      "loss: 0.111559  [38464/60000]\n",
      "loss: 0.111559  [38464/60000]\n",
      "loss: 0.034809  [44864/60000]\n",
      "loss: 0.034809  [44864/60000]\n",
      "loss: 0.110702  [51264/60000]\n",
      "loss: 0.110702  [51264/60000]\n",
      "loss: 0.332763  [57664/60000]\n",
      "loss: 0.332763  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.103586 \n",
      "\n",
      "Done!\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.103586 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_dataloader, model, loss_fn, optimizer, writer, t)\n",
    "    test(test_dataloader, model, loss_fn, writer, t)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "354b131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `CNN([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `CNN([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 3 of general pattern rewrite rules.\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 3 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "source": [
    "example_inputs = (torch.randn(1, 1, 28, 28),)\n",
    "model.to('cpu')\n",
    "onnx_program = torch.onnx.export(model, example_inputs, dynamo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "d166f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "1246d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "6c169926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30132), started 5:09:26 ago. (Use '!kill 30132' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d05d4f4d54a1e5b3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d05d4f4d54a1e5b3\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
